
ffmpeg 的学习过程笔记，会往其中放思维导图。

1.解码（将mp4文件视频数据解码成yuv等原始数据，对应的思维导图是 mydecode.png）：
AVFormatContext ：封装格式的容器 e.gMP4
AVCodecContext ：编解码中的相关数据，编解码格式的抽象
AVCodec ：存储编解码器的结构体，编解码器
AVStream 对流的抽象
AVFrame ：用来存储原始数据 视频的就是yuv，音频的就是pcm
AVOutputFormat:对输出文件格式的抽象

如果视频原始数据AV_PIX_FMT_YUV420P 就是平面格式的（yyyy uu vv yv12） ,(yyyy uu vv nv21)。
但是我的锤子手机使用yv12最后拿到的yuv原始数据是nv12才能正确解析出来。
当如果发现你解析出来的界面蓝绿色对调了问题就是你的uv数据放反了。
如果是花屏了就是你的原始数据和解码器对应错了。比如你yv12的原始yuv数据，然后用nv12来解析，就是花屏

其他：malloc_usable_size（*）查看实际分配的内存大小


2.编码：（将yuv数据编码成h264文件，对应的思维导图是 my_encode_yuv_2_h264.png）
这里记录下导图中没有体现的细节
通过avformat_alloc_context创建的AVFormatContext
通过av_guess_format创建AVOutputFormat
使用   AVFormatContext->oformat = AVOutputFormat;来关联
avpicture_fill，为AvFrame来关联一个缓冲区，然后Avframe中的data[4]其实就是指向这个缓冲区的不同地方。要看你是原始数据的格式方式。


3.关于ffmepg中pts，dts，time_base中的问题：
先以视频帧来理解
pts就是视频显示的时间
dts就是被解码的时间
time_base ，就是一秒钟的一个，比如视频是一秒钟25帧，那么time_base就是=（1,25）=1/25;
如果是吧时间分成90000份，那么就是1/90000秒，此时的time_base={1,90000};
所以pts就是占多少个时间格子
在ffmpeg中有av_q2d(time_base),就是把分数转换成小数的
pts * av_q2d(time_base)才是帧的显示时间戳。
AVStream->duration就是这个视频流有多长
AVStream->duration * av_q2d(time_base) = time就是这个视频的长度（秒）
ffmpeg内部的时间和标准的时间转换就是一个AV_TIME_BASE
比如ffmepg内部的时间 = time * AV_TIME_BASE;

av_rescale_q(int64_t a, AVRational bq, AVRational cq) 这个是时间基的转换函数a * bq / cq
比如计算的时间他会有avcodec的时间基，读取的instream有时间基，写入文件的时候有outstream的时间基等等。都需要转换


4.音频录制（通过设备拿到pcm原始数据转换成aac存在本地 , 导图是audio_record.png）：
http://blog.csdn.net/XIAIBIANCHENG/article/details/72810495 格式转换
音频录制，首先要通过android上层的api（audiorecord）拿到数据，不过在jni成也能拿到数据.
在上层会初始化一些参数，我这边是单声道，44100的采样率，16bit的数据这是大多数的android机都支持的。
但是在ffmepg自带的编码器（当然可以在编译ffmpeg中支持对应的编码器，aac等）中是只支持AV_SAMPLE_FMT_FLTP。
所以在底层进行转换一下。但是如果是双声道的话，两者区别不大(但是不知道用AV_SAMPLE_FMT_FLTP来编码AV_SAMPLE_FMT_S16是否会成功)。
下面看看双声道
AV_SAMPLE_FMT_S16（data[0] lrlrlrlrlrlr）
AV_SAMPLE_FMT_FLTP(data[0] data[1] lllll rrrrr);
这个就要用SwrContext来转换下。具体内容看代码my_audio_record.c

因为是要转码所以bufferSizeInBytes 要设置2048，如果是双声道的话要设置成4096.
frame->nb_samples：采样样本个数（1024），一帧的采样个数
ac->sample_rate：采样率（e.g 44100 就是每秒44100个sample)，一秒采样次数
那么一帧的数据量就是 frame->nb_samples（1024） * 声道数（2） * av_get_bytes_per_sample(s16)(2) = 4096
音频的时间方面：
一帧 1024个 sample。采样率 Samplerate 44100KHz，每秒44100个sample, 所以根据公式   音频帧的播放时间=一个AAC帧对应的采样样本的个数/采样频率
当前AAC一帧的播放时间是= 1024*1000000/44100= 22.2ms(单位为ms)
调试发现，对音频重采样后，frame_->format中的格式并不会改变，所以使用av_get_bytes_per_sample获取每个采样的大小的时候需要手动改成outFormat


5.采集音视频并复合mp4（现在已经做好的还有点问题就是音频播放过快。但是还不知道如何改正）
这里面需要注意的一个点是av_compare_ts ，传入的是上一帧（音视频）的pts，time_base 是最后状态的pts，
比如音频是编码后的codec->time_base , 视频是编码后的video_stream->time_base.
这个音视频同步的问题感觉还没有个方向，先暂时放一会（xxxxxxxxxx）


6.音视频同步问题。
1.一般是视频去同步音频，因为音频有卡顿的话体验很差。
2.音视频两者的时间基不同，需要转换下才能比较两个的pts的时间
3.如果视频播放过快。需要sleep等待下音频












